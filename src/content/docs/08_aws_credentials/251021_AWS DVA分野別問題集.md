---
title: AWS DVA分野別問題集
publishedDate: 2025-10-21
---

# AWS DVA分野別問題集

## DVA-1.2-AWS Lambda 用コードの開発

Q1 【DVA-10】ある SaaS ベンダーが、社内スタッフと指名されたテスターだけに 1 本の JSON ベース REST API をクローズドベータとして公開する計画を立てています。

- トラフィックは 1 日あたりおよそ 60~70 リクエストと少なく、向こう 90 日は大きな変動を見込んでいません。
- バックエンドの呼び出し回数と費用を抑えるため、AWS が提供するレスポンスキャッシュ機能を利用したいと考えています。
- 従量課金で運用をできるだけ簡素化し、サーバーやコンテナの常時稼働コストは避けたいという要件があります。
- スタックは AWS SAM など IAC ツールで定義してデプロイしてもよいことになっています。

これらの条件を最小コストで満たすアーキテクチャとして、最も適切なものはどれですか。

1. AWS App Runner に最小プロビジョン 1 コンテナのサービスを作成し、CloudFront を前段に置いてレスポンスをキャッシュさせる。
2. Amazon API Gateway (REST API)をフロントに配置し、統合キャッシュを有効化したうえで AWS Lambda 関数をバックエンドとする構成を SAM テンプレートで展開する。
3. Amazon CloudFront をオリジンとして設定した Application Load Balancer から Lambda ターゲットを呼び出し、キャッシュは CloudFront に任せる。
4. Fargate 1 ノードのみの Amazon EKS クラスターを構築し、API Pod と同一タスク内に Redis を同梱してアプリケーション側でキャッシュを実装する。

A1 2

該当選択肢が適切である理由

- API キーを持っている人だけが対応する API を使用することができる
- Usage Plan: API キーを使う人ごとに「どのぐらい使っていいか決める仕組みのこと」
- AWS SAM（Serverless Application Model）とは、**サーバーレスアプリケーションを簡単に構築・デプロイするためのフレームワーク**。Lambda、API Gateway、DynamoDB、S3、Step Functions などのサーバーレス構成をコード（YAML/JSON）で定義する。

  ※ **SAM は CloudFormation のラッパー（簡略版）**。CloudFormation で書くと長くなる定義を、SAM では短く書ける

他の選択肢が不適である理由

- App Runner + CloudFront: App Runner の最小プロビジョニング(1 コンテナ)が恒常費。超低トラフィックには割高
- CloudFront → ALB → Lambda: ALB は常時稼働の固定費が発生。低トラフィックに不向き。キャッシュは CloudFront 任せでも ALB 維持費がボトルネック
- EKS(Fargate)＋ Redis 同梱: EKS のコントロールプレーン費用や構築・運用の複雑さが過剰。要件の「運用簡素化」「最小コスト」に反する

Q2 【DVA-11】社内のレポート閲覧アプリでは、モバイル端末から TIFF ファイルを Amazon S3 バケット reports-source/に直接アップロードしています。
開発チームは次の拡張を求めています。

- アップロード完了と同時にファイルを縮小し PDF に変換、その結果を reports-pdf/ プレフィックスに保存する
- アップロードの待ち時間を伸ばさず、既存のアップロードコードや既存 Lambda 関数はそのままにする
- リアルタイムで処理し、キューやバッチなど追加サービスを導入しない

これらの要件を最も満たすアーキテクチャはどれか。

1. S3 の ObjectCreated イベントを新規の AWS Lambda 関数へ直接通知し、その関数で TIFF を縮小·PDF 化したうえで reports-pdf/に保存する。
2. S3 から Amazon SQS キューヘイベントを流し、そのキューをポーリングする新しい Lambda 関数で変換して PDF を保存する。
3. 既存の Lambda 関数のコードに変換ロジックを追記し、変換後の PDF を別プレフィックスへ書き出す。
4. Amazon EventBridge で 3 分間隔のスケジュールルールを作成し、直近のアップロードを走査·変換するバッチ用 Lambda 関数を起動する。

A2 1

該当選択肢が適切である理由

- アップロード待ち時間に影響なし：モバイル端末は S3 に直接 PUT するだけ。S3 の ObjectCreated イベントで別の新規 Lambda が非同期起動されるため、アップロード完了を待たずに処理へ移れる
- 既存コード／既存 Lambda を変更しない：新規 Lambda をイベント購読させるだけで要件を満たせる
- リアルタイム処理 & 追加サービスなし：S3→Lambda のイベント駆動で即時に変換。SQS やバッチ(EventBridge)は不要

他の選択肢が不適である理由

- Amazon SQS キューヘイベント: キューなどの追加サービスを導入しないと記載あるため + Lambda のポーリングは最短でも数秒遅延が入るためリアルタイム性も劣る
- 既存の Lambda 関数のコードに変換ロジックを追記: 既存 Lambda 関数はそのままにすると記載あるため
- Amazon EventBridge で 3 分間隔のスケジュールルール: リアルタイム性を満たしていない。バッチ処理になっている

Q3 【DVA-12】開発部門の Lambda 関数では、実行の途中で約 1.2 GB の一時データを生成します。計算が終わればそのデータは破棄して構わず、永続保管の義務はありません。最小コストと高速処理を最優先とし、別サービスとの連携や追加リソースの管理は極力避けたい場合、最適な実装はどれでしょうか。

1. Lambda の/tmp 領域をワークスペースとして利用し、Ephemeral storage を 2GB に拡張して処理後は自動的に上書きされるようにする。
2. 作成したファイルを Amazon S3 Glacier Instant Retrieval にアップロードし、タグ付けで 24 時間後に削除させる。
3. AWS Fargate タスクを起動して Amazon EBS ボリュームをアタッチし、タスク終了時にファイナライザでデータを削除する。
4. 関数に Amazon EFS アクセスポイントをマウントし、完了後に EventBridge ルールで手動クリーンアップを実行する。

A3 1

該当選択肢が適切である理由

- Lambda にはデフォルトで 512MD の/tmp エリアが用意されている。これを Ephrmeral storage（エフェメラル・ストレージ）と言い、設定で最大 10GB まで拡張可能
- 関数の終了やコンテナ再利用時の上書きでデータが自動的に消えるため、削除ロジックを組む必要が無い。同じインスタンスが再利用される間のみ有効
- 同じマシン内ストレージとなるため、S3 や EFS より高速

他の選択肢が不適である理由

- Amazon S3 Glacier Instant Retrieval: そもそも長期アーカイブ用で、早期削除ペナルティが発生
- Amazon EFS + EventBridge: 単発で破棄可能なデータにはオーバースペック
- AWS Fargate + Amazon EBS: 長時間・大容量処理には有効ではあるが今回はオーバースペック

Q4 【DVA-13】CSV レポートを生成する AWS Lambda 関数があります。1 回の実行中に複数回アクセスする最大 470 MB 程度の一時ファイルを作成しますが、処理が終われば残しておく必要はなく、ほかの呼び出しから参照されることもありません。料金と処理遅延を最小に抑えるため、ファイルの保管先として最適なのはどれですか。

1. 関数に Amazon EFS をアタッチし、/mnt/data 配下へ書き込む
2. Lambda の実行コンテナ内に用意されている/tmp フォルダに保存する
3. デプロイコードが置かれる/var/task ディレクトリを直接書き込み先にする
4. 毎回 Amazon S3 バケットヘアップロードし、読み取り時にダウンロードする

A4 2

該当選択肢が適切である理由

- 1 回の呼び出しだけで使い捨てる最大 512 MB(※デフォルト値)のエフェメラルストレージ領域が/tmp に自動的に用意されている
- ネットワーク越しの I/0 が発生しないため待ち時間が最小で、追加料金もかからない

他の選択肢が不適である理由

- EFS: マウント/ネットワーク通信に伴うレイテンシと GB·転送量に応じた追加コストが発生
- デプロイコードの/var に書き込み: /var/task はデプロイされたコードを保持する領域であり実行時は読み取り専用。書き込みを試みるとアクセスエラーになる。Lambda のファイルシステム特性を理解していればこの選択肢は除外できる。もし迷った場合は「/tmp 以外への書き込み可否」を再確認
- S3: 今後残しておく必要のないデータを、わざわざ登録・読み出しする料金を払って、ネットワーク往復による遅延まで発生させて外部オブジェクトストレージに置くのは非効率

Q5 【DVA-14】Python 3.11 で実装した AWS Lambda 関数が、スマートホームの温湿度データを Amazon RDS for PostgreSQL に書き込んでいます。関数は 1 秒あたり約 2,000 回呼び出され、ハンドラーの内部で毎回 psycopg2.connect()を実行しているため、コネクション確立に時間がかかり実行時間と費用が膨らんでいます。数行のコード変更で済み、同時にコスト削減とスループット向上を狙える最も適切な対応はどれでしょうか?

1. データベースを Amazon DynamoDB に置き換え、接続オーバーヘッドを排除して書き込みレイテンシーを下げる。
2. Lambda のメモリを 128 MB から 1024 MB へ増やし、割り当て vCPU を増加させて処理全体を高速化する。
3. ハンドラーの外側でデータベースコネクションプールを初期化し、同一実行環境の後続呼び出しで既存接続を再利用できるようにする。
4. RDS インスタンスを db.r6g.large から db.r6g.4xlarge へ拡張し、CPU とメモリを増やして接続時間を短縮する。

A5 3

該当選択肢が適切である理由

- Lambda は同一実行環境が“ウォーム”な間は再利用されるため、ハンドラー外のグローバル領域で DB コネクション（またはコネクションプール）を初期化しておくと、後続呼び出しで接続確立を省ける
- 数行の変更でコールド以外の接続オーバーヘッドを大幅に削減でき、実行時間＝費用の両方を下げられる

他の選択肢が不適である理由

- DynamoDB: 全データモデルやクエリを変える大仕事となり、数行のコード変更では済まない
- Lambda のメモリ増強: メモリを増強しても接続確立の時間短縮はあまりできないし（そもそもの接続確立回数を減らした方がいい）、メモリが増強されている分コストがかさむ
- RDS の CPU・メモリ増強: CPU・メモリを増強しても接続確立の時間短縮にはならない

Q6 【DVA-15】オンライン教育プラットフォームでは、受講申込を受け付けるサーバーレス API を AWS 上に展開する計画です。開発チームは高スループットを狙い、Swift 言語で処理ロジックを実装しました。しかし現行の AWS Lambda 公式ランタイムには Swift が用意されていません。インフラは 1 つの AWS SAM スタックで統一管理し、API Gateway から呼び出される申込処理関数も同スタック内でリリースしたいと考えています。最適なデプロイ戦略はどれでしょうか。

1. Swift 用のカスタムランタイムー式を含む新しい Lambda Layer を SAM テンプレートで定義し、関数の Runtime を provided.al2 に設定してその Layer を参照させ、sam deploy する。
2. Swift アプリケーションをコンテナ化し、SAM の ImageUri に指定してデプロイするが、追加の Lambda Layer は用意しない。
3. Swift バイナリだけを S3 に置き、SAM テンプレートで Runtime を provided.al2 にするが、カスタム Layer は指定しないまま sam deploy する。
4. ビルドした Swift バイナリを S3 に配置し、SAM テンプレートで Runtime を nodejs18.x と宣言して Lambda 関数を作成し、sam deploy を実行する。

A6 1

該当選択肢が適切である理由

- 未サポート言語＝カスタムランタイムを理解しているかを問う典型問題
  → Runtime: provided.al2 を指定し、bootstrap と Swift ランタイム一式を Layer で配布するのが教科書どおり
- SAM で Layer と関数を同一スタック管理でき、複数関数で共有も可能（保守・コスト ◎）
- 関数コード側は Swift ビルド済みハンドラだけにでき、CI/CD もシンプル

他の選択肢が不適である理由

- Swift アプリを Docker イメージとしてビルドし、そのイメージを Amazon ECR にプッシュ。そして SAM テンプレートの ImageUri に指定して Lambda にデプロイする → これにより、Lambda は「Swift ランタイム入りのコンテナ」を起動して関数を実行できるが、

  - イメージ（Docker）は ZIP とは別管理
  - SAM テンプレートに書けても、SAM だけでは完結デプロイできない
  - だから「1 つの SAM スタックで統一管理したい」という要件とは相性が悪い

- Runtime を provided.al2 にするが、カスタム Layer は指定しない: bootstrap が無いため、起動不可。Lambda は「どう実行するか」を知らないので、Exec format error で失敗する。カスタムランタイムには必ず bootstrap ＋依存ライブラリが必要
- Runtime を nodejs18.x にして Swift バイナリを実行: ランタイムとバイナリが不一致。Node.js ランタイムには Swift 実行環境がなく、Lambda がハンドラを見つけられない。Handler not found エラーになる

※ 補足 provided.al2 は Amazon Linux 2 (AL2) ベースの最小環境
※ 補足 「bootstrap」は、Lambda がプログラムを呼び出すための**最初の命令書（起動スクリプト）**。公式ランタイムがない言語では、このファイルが“ランタイムそのもの”になる

| 状態                                                  | Lambda が実行できる？ | 理由                                                         |
| ----------------------------------------------------- | --------------------- | ------------------------------------------------------------ |
| S3 に置いた Swift バイナリ（直接参照）                | ❌                    | Lambda は S3 を実行パスとして扱えない                        |
| S3 に ZIP パッケージをアップロードし、`CodeUri`で指定 | ✅                    | Lambda が ZIP を展開して実行環境を構築し、bootstrap から実行 |
| ECR に Docker イメージを登録して`ImageUri`で指定      | ✅                    | Lambda が ECR から pull してコンテナとして起動               |

Q7 【DVA-16】あなたはスマートホーム機器から届く測定値を処理する 3 つの AWS Lambda 関数(IngestFn、ArchiveFn、NotifyUserFn)を作成しました。これらの関数は全て共通の Amazon S3 バケット名と DynamoDB テーブル名を参照しています。将来的にリソース名が変更されても、ソースコードの修正や再デプロイを行わずに値を差し替えられる運用にしたいと考えています。Lambda の設定管理として最も適切な方法はどれでしょうか。

1. バケット名とテーブル名を JSON ファイルにまとめて Lambda レイヤーに格納し、名称が変わるたびに新しいレイヤーバージョンを組み込む
2. 各 Lambda に同じキー名で環境変数を設定し、バケット名とテーブル名を実行時に参照させる
3. 名称をハードコーディングした新しい Lambda バージョンを発行し、加重エイリアスで段階的に切り替える
4. DynamoDB Streams を有効化し、最新のリソース名をストリーム経由で関数へ配信する

A7 2

該当選択肢が適切である理由

- Lambda には実行環境ごとにキー/バリュー形式の環境変数を設定できます。環境変数はマネジメントコンソール、AWS CLI、CloudFormation などからコード再デプロイなしで更新が可能
- 3 つの関数でキー名を統一しておけば、運用担当者は 1 箇所で値を変更し、それぞれの関数は実行時に最新値を取得できる

他の選択肢が不適である理由

- JSON ファイルにまとめてレイヤーに格納: レイヤーは複数関数で共通ライブラリや設定を共有できる反面、新しいレイヤーバージョンを作成したあとに各関数の設定を更新して再デプロイしなければ反映されない
- 名称をハードコーディング: 「設定は外出しして、コードには埋め込まない」を原則とすること
- DynamoDB Streams を有効化し、最新のリソース名をストリーム経由で関数へ配信: DynamoDB Streams はテーブル内アイテムの変更イベントを通知する仕組みであり、システム設定値を配布する用途ではない

Q8 【DVA-17】分析チームは注文データを保存する DynamoDB テーブルを AWS Lambda から呼び出しています。今後テーブル名が変わる可能性があり、ソースを編集したり ZIP を作り直したりせずに即座に反映できる、最もシンプルで運用負荷の少ない方法を選んでください。

1. テーブル名を AWS AppConfig に保存し、リクエストごとに SDK でフェッチする
2. テーブル名を書いた JSON ファイルを S3 に置いてレイヤー化し、変更時にレイヤーを新バージョンで再デプロイする
3. テーブル名をコードにハードコーディングし、変更のたびに関数をビルドし直してバージョンを更新する
4. テーブル名を Lambda の環境変数として登録し、関数内では process.env で読み取る

A8 4

該当選択肢が適切である理由

- Lambda には「環境変数」をコンソールや laC で簡単に設定でき、更新も即時反映される
- コードや ZIP を改変する必要がなく、デプロイ不要でランタイムの設定だけを変更できるため、最小限の運用負荷でテーブル名変更に対応できる
- メモリにロード済みのコンテナであっても、次の初期化時には新しい環境変数が使われるため実質リアルタイムに切り替えられる
- 環境変数は標準機能なので追加コストが発生しない

他の選択肢が不適である理由

- AWS AppConfig に保存し、SDK でフェッチ: やれなくはないが、呼び出しごとに設定を取得すればレイテンシと料金が発生。AppConfig の強みは段階的リリースや複数環境管理にあるが、単一のテーブル名を変更するだけなら環境変数で十分
- S3+レイヤーという二段構成は単なる文字列設定には過剰。レイヤーはコード共有用、頻繁に変わる小さな設定は環境変数
- 設定値のハードコーディングは NG。コードと設定を分離するクラウドネイティブ設計原則

※ 補足 `process.env`は、Node.js のプログラムの中で環境変数を読み取るためのオブジェクト（Python だと`os.environ`）

Q9 【DVA-18】スマートフォン向けソーシャルアプリのユーザー情報を保存している DynamoDB テーブル「UserAccount」。アイテムの追加·変更·削除が行われた際に数秒以内で AWS Lambda を呼び出し、監査イベントを記録したい。サーバー運用や定期的なポーリングを行わず、フルマネージドサービスだけで実装する必要がある。最適なアーキテクチャはどれか。

1. UserAccount で DynamoDB Streams を NEW_AND_OLD_IMAGES モードで有効化し、そのストリームを Lambda のイベントソースとして関連付ける
2. UserAccount テーブルを Kinesis Data Firehose と統合し、Firehose ストリームを Lambda のトリガーにする
3. DynamoDB Streams を KEYS_ONLY に設定しておけば、追加設定をしなくても Lambda が自動的に呼び出される
4. AWS Glue ジョブを 5 分ごとに実行して UserAccount テーブルを全件エクスポートし、S3 への書き込みを契機に Lambda を起動する
5. Amazon EventBridge に 30 秒間隔のスケジュールルールを作成し、UserAccount を Scan して結果を Lambda に渡す

A9 1

該当選択肢が適切である理由

- DynamoDB Streams は DynamoDB テーブルに対する挿入·更新·削除イベントを最短数ミリ秒で配信できるフルマネージドサービス
- NEW_AND_OLD_IMAGES を有効にすると、変更前後のアイテム全体がストリームに含まれるため、監査用途で「誰が」「いつ」「何を」変更したかを詳細に記録できる
- Streams はネイティブに AWS Lambda と統合されており、ポーリングやサーバー管理を行うことなくイベントドリブンで関数を起動できる

他の選択肢が不適である理由

- Firehose は主にデータをストレージ(S3、OpenSearch、Redshift など)ヘバッファリングして配信するサービスで、Lambda トリガーとして直接機能しない

## DVA-1.2-アプリケーション開発でのデータストアの使用

Q1 【DVA-26】オンライン書籍ストアを運営する Libra Corp.では、Aurora MySQL で構成された1つのDB クラスター(プライマリ1台+レプリカ2台)を使用しています。
·購入確定API は1秒あたり数千件の大量書き込みが発生するため、m6g.largeインスタンスに割り当てたい。
·社内の設定ダッシュボードは更新頻度が低く、コスト削減の観点からt4g.medium インスタンスに載せたい。
両ワークロードとも書き込み権限が必要ですが、アプリケーション側で接続先を逐次切り替える実装は避けたいと考えています。
費用を抑えながらワークロードを分離する最適な設計はどれですか。

1. 設定ダッシュボードはリーダーエンドポイント経由で更新処理を行い、購入確定APIはライターエンドポイントに接続する。
2. 設定ダッシュボード用に新しい Aurora クラスターを作成し、t4g.medium インスタンスを配置する。購入確定API は既存クラスターのライターエンドポイントを利用する。
3. m6g.large とt4g.medium をターゲットにした2つの書き込み可能なカスタムエンドポイントを作成し、購入確定 API はhigh-write エンドポイント、設定ダッシュボードは low-cost エンドポイントに接続させる。
4. すべてのトラフィックをクラスタエンドポイントに送信し、プライマリインスタンスを m6g.large のみで運用する。

A1 3

該当選択肢が適切である理由

- Aurora MySQL クラスターを Amazon RDS Proxy に関連付けると、ターゲットグループ単位で「書き込み可能」なカスタムエンドポイントを作成できます。
·high-write ターゲットグループには m6g.large インスタンスだけを登録し、そこで大量書き込みを集中処理。
·low-cost ターゲットグループには t4g.medium インスタンスだけを登録し、低頻度の更新を処理。
- エンドポイントレベルでワークロードを自動的に分離しつつ、RDS Proxy がコネクションプーリングと自動フェイルオーバーを担うため、アプリケーション側の実装変更はエンドポイントの切替だけで済みます。- 追加クラスターを用意しないためストレージやバックアップの重複コストも発生しません。

他の選択肢が不適である理由

- リーダーエンドポイント経由で更新処理、購入確定APIはライターエンドポイントに接続: リーダーエンドポイント配下のインスタンスは**読み取り専用**。UPDATE/INSERT/DELETE を発行するとエラーが返るため、書き込み権限が必要という要件を満たしません。これを選んだ方は「ライター/リーダーエンドポイントの違い」と「Auroraレプリカが原則Read-Onlyである理由」を復習してください。
- すべてのトラフィックをクラスタエンドポイントに送信し、プライマリインスタンスを m6g.large のみで運用: 高負荷の購入確定APIと低負荷の設定ダッシュボードを同じインスタンスに集約すると、ピーク時に CPU·IOが競合しレスポンス悪化を招きます。またt4g.medium を活用しないためコスト削減にもなりません。「ワークロード分離」と「インスタンスクラスのサイズミスマッチ」の観点が不十分です。
- 設定ダッシュボード用に新しい Aurora クラスターを作成し、t4g.medium インスタンスを配置する。購入確定APIは既存クラスターのライターエンドポイントを利用: クラスターを増やすとデータを二重で保持するためストレージ費用とバックアップ費用が倍増し、運用も複雑になります。クロスクラスタでデータを同期

Q2 【DVA-27】語学学習プラットフォーム「LinguaFlex」は、iOS·Android·Webの各クライアント間で単語帳の進捗状況や UI プリセットを即時同期させたいと考えています。運営チームは独自APIやサーバーを保守するリソースがなく、完全マネージドな手段を希望しています。ログイン処理や外部 ID プロバイダー連携はすでに別モジュールで実装済みで、本件では扱いません。データが更新されたら全端末にほぼリアルタイムで反映されることが絶対条件です。要件をもっとも満たすAWS サービスはどれでしょうか。

1. Amazon DynamoDB Global Tables
2. Amazon Cognito User Pools
3. AWS AppSync (旧Amazon Cognito Sync からの移行推奨)
4. Amazon API Gateway WebSocket API と AWS Lambda で構成する自前同期

A2 3

該当選択肢が適切である理由

- AppSync はフルマネージドの GraphQL サービスで、モバイル/Web クライアント向けにリアルタイムのデータ同期とオフライン機能を提供。
- クライアント SDK により、サブスクリプション機能でバックエンドの変更を即時プッシュし、全端末がほぼリアルタイムで同じ状態を保てます。
- 基盤サーバーや WebSocketの管理は不要で、DynamoDB など既存のデータソースとシームレスに連携できるため、「運営チームに保守リソースがない」「完全マネージド」「リアルタイム同期」という要件をすべて満たします。

他の選択肢が不適である理由

- Amazon Cognito User Pools: Cognito User Pools は認証·ユーザーディレクトリを提供するサービスであり、データ同期やリアルタイム通知の機能は持ちません。すでに「ログイン処理や外部ID プロバイダー連携は別モジュールで実装済み」となっている本シナリオでは、進捗データを端末間で同期する課題を解決できないため不適切です。ここを選択した場合は、Cognito の認証機能とデータ同期機能(旧Cognito Sync)を混同している可能性があるため、サービスごとの役割を整理しましょう。
- Amazon DynamoDB Global Tables: Global Tables はマルチリージョンレプリケーションを実現するバックエンド機能です。テーブル間でデータは自動同期されますが、クライアントアプリにはプッシュされません。つまり、端末側が定期的にポーリングしない限り「ほぼリアルタイム」にはなりません。また、API や認可レイヤを別途用意する必要があり、完全マネージドという要件にも合致しません。この選択肢を選んだ方は「バックエンドの多リージョン冗長」と「クライアント間リアルタイム同期」の違いを復習してください。
- Amazon API Gateway WebSocket API と AWS Lambda で構成する自前同期: WebSocket と Lambda を組み合わせれば確かにリアルタイム通信基盤を構築できますが、接続管理、メッセージルーティング、スケーリング、権限制御などを自前で実装·運用する必要があります。問題文で求められている「完全マネージド」「保守リソースがない」という条件を満たしません。実務では、カスタムプロトコルや特殊要件がある場合に有効ですが、今回の一般的なデータ同期にはオーバーエンジニアリングです。

·この問題の論点
この問題は、モバイル/Web アプリ間でのリアルタイムデータ同期と、それをフルマネージドで実現できる AWS サービス選択に関する知識を問う問題です。AppSyncの機能と位置づけ、Cognito User Pools との役
割の違い、バックエンドレプリケーションとクライアント同期の区別、WebSocketを用いた自前実装との比較を理解しているかがポイントになります。

Q3 【DVA-28】社内の進捗共有ツールでは、複数の AWS Lambda が同じ「Workltem」レコードをほぼ同時に UPDATE します。
最初に取得した内容を基にコメントを追記したい開発者は、他プロセスの書き込みで自分の変更が消えないようにしたいと考えています。
外部キューやロックテーブルを増やさず、現在と同じくDynamoDBMapperでPOJO をマッピングする前提で、最も手軽に競合を避ける方法はどれですか。

1. Amazon ElastiCache for Redis で分散ロックを取り、解放後に DynamoDBに書き込む構成へ変更する
2. Workltemに「rev」フィールドを追加し、@DynamoDBVersionAttribute を付けて DynamoDBMapper の楽観ロック機能を有効化する
3. DynamoDB Streams から EventBridge ヘイベントを送り、書き込み後に差分チェックして衝突がある場合は後続の Lambdaを再実行する
4. TransactWriteltems で同じアイテムを2回読み書きし、値が変わっていればループでリトライする独自ロジックを実装する

A3 2

該当選択肢が適切である理由

- DynamoDBMapper はエンティティに@DynamoDBVersionAttribute を付与すると、自動的にバージョン番号(Revision)を管理し、読み取り時点の値と書き込み時点の値が一致しない場合には ConditionalCheckFailedException を発生させます。
- アプリケーション側は例外を受け取ってリトライするだけで“後勝ち”の上書きを防げるため、外部ロックや追加テーブルは不要です。すでにPOJO を DynamoDBMapperで扱っている環境に最小変更で組み込めることから、手軽さと安全性のバランスが最も高い解決策になります。

他の選択肢が不適である理由

- Amazon ElastiCache for Redis で分散ロックを取り、解放後に DynamoDB に書き込む構成へ変更する: 分散ロックは確実に競合を抑止できますが、キャッシュクラスターの構築·監視·運用が新たに発生し、「最も手軽」という要件から外れます。可用性やフォールトトレラントな実装(Redlockなど)を正しく組むのも簡単ではありません。DynamoDB本体に備わる機能で十分に対処できるため、この方法を選ぶ必要性は低いでしょう。これを選択した方は「楽観ロック」と「悲観ロック(外部ロック)」のコスト差を整理しておきましょう。
- TransactWriteltems で同じアイテムを2回読み書きし、値が変わっていればループでリトライする独自ロジックを実装する: トランザクションAPIは確かにアトミック性を保証しますが、ここでは重複する読み書きを自前でループさせる必要があり、コード量が増えるうえにリードキャパシティも余計に消費します。さらに、トランザクションは1秒あたりのリクエスト数制限があるためスループット面でも不利です。DynamoDBMapper のバージョニングなら同等の競合検知をより簡単に実装できるので、手軽さの観点で劣ります。この選択肢を選んだ方は「Mapperの条件付き書き込み」と「TransactWriteltems」の適材適所を復習してください。
- DynamoDB Streams から EventBridge ヘイベントを送り、書き込み後に差分チェックして衝突がある場合は後続の Lambdaを再実行する: Streams を介した非同期ワークフローは後処理や監査には適していますが、「書き込む前に衝突を防ぐ」という目的には合致しません。更新後に検知してリトライするため、結局は一度失敗した状態がデータベースに存在してしまい“ロストアップデート”を防げないケースがあります。構成も複雑になり、レイテンシも伸びるため「最も手軽」とは言い難い手法です。選択した方は「同期的ロック」と「後処理型衝突解決」の違いを確認しておきましょう。

·この問題の論点
この問題は DynamoDB における同ーアイテムへの同時更新競合をどのように防ぐか、特に「DynamoDBMapperが提供する楽観ロック機能」と「外部ロックやトランザクションとの比較」についての知識を問う問題です。

Q4 【DVA-29】稼働中の DynamoDB テーブル ActivityLog は userld をパーティションキー、eventAt をソートキーとしている。新たに level とzone の組み合わせで高頻度に検索するユースケースが追加された。テーブル定義はそのままにし、アプリケーション側の修正だけで低レイテンシかつ低コストを実現したい。この要件に最適なインデックス戦略を選択せよ。

1. level をパーティションキー、zone をソートキーとする GSIを追加し、その GSIへQuery を行う
2. level と zone を主キーに持つ新規テーブルを用意し、DynamoDB Streams + Lambda でActivityLog の更新を複製する
3. level とzone を FilterExpression に指定した Scan を用い、オンデマンドキャパシティで費用を抑制する
4. level と zoneの2項目を複合パーティションキーとした LSI を作成し、強整合性で検索する

A4 1

該当選択肢が適切である理由

- 既存テーブルに後付けでインデックスを追加できるのは Global Secondary Index(GSI)だけです。
- GSI では元のテーブルとは独立したパーティションキー/ソートキーを定義できるため、level を均等に分散するパーティションキーにし、zone をソートキーにすることでKeyConditionExpression を使った Query が可能になります。
- Query は内部的にハッシュパーティションを1つだけ読み取るため低レイテンシですし、読み取り·書き込みともにGSI に対して課金されるので、アクセスパターンがはっきりしている場合は総コストも抑えやすいです。
- テーブルスキーマを変えずにアプリケーション設定だけで実現できるという要件にも合致します。

他の選択肢が不適である理由

- level とzoneの2項目を複合パーティションキーとした LSIを作成し、強整合性で検索する: Local Secondary Index (LSI)は作成時点でしか定義できず、すでに稼働しているテーブルには追加できません。また LSIは元のパーティションキー(ここではuserld)を共有する決まりがあるため、level とzoneをパーティションキーにすること自体が不可能です。さらに10 GB制限もあるため、大規模テーブルでは実用的ではありません。この選択肢を選んだ方は「後付け可能なのは GSI、既存パーティションキーを共有するのがLSI」という仕様の違いを再確認しましょう。
- level と zone を主キーに持つ新規テーブルを用意し、DynamoDB Streams+LambdaでActivityLogの更新を複製する: 技術的には要件を満たせますが、ストリームと Lambdaの構成·運用が追加で必要になり、二重書き込みによる整合性遅延や追加コストが発生します。「テーブル定義はそのままにし、アプリケーション側の修正だけで」とあるので過剰であり要件逸脱です。学習上は「Streams 複製」は複雑さとコストのトレードオフである点を覚えておきましょう。
- level とzone を FilterExpression に指定した Scanを用い、オンデマンドキャパシティで費用を抑制する: Scanはテーブル全分割を順に読み取るため、アイテム数が増えるほどレイテンシもコストも跳ね上がります。FilterExpression は読み取り後にサーバ側で除外を行うだけなので無駄なIOが減りません。オンデマンドキャパシティを選んでも「単価×読み取り量」は変わらないため、条件にある高頻度アクセスでは費用が高騰します。この選択肢を選んだ場合は「Query とScanの違い」「FilterExpressionがキー条件を置き換えない」という点を復習してください。

·この問題の論点
この問題は DynamoDB インデックス戦略に関する知識を問うものです。主に「GSIとLSI の機能差·作成タイミング」「後付け可能かどうか」「Query とScanのレイテンシ/コスト特性」「Streams+Lambdaレプリケーションの運用コスト」といった理解が求められます。

Q5 【DVA-30】モバイルゲームスタジオが、DynamoDB テーブルをプロビジョンドキャパシティーモードで利用している。
各プレイヤーステータス項目のサイズは約3.5KBで、アプリケーションは毎秒200件の強い整合性読み込みを行わなければならない。
このトラフィックをまかなうために設定すべき最小の読み取りキャパシティーユニット(RCU)はどれか。

1. 225 RCU
2. 400 RCU
3. 200 RCU
4. 190 RCU

A5 3

該当選択肢が適切である理由

- DynamoDB では、強い整合性読み込みの場合「1RCU=4KBまでの項目を1秒間に1件読み込む」能力に相当します。
- 問題の項目サイズは約3.5KBと4KB未満であるため、1件あたり1RCUで済みます。
- 毎秒200件の読み込みを処理するには200 RCUが必要となり、これが最小値です。

他の選択肢が不適である理由

- 190 RCU: 1件1RCUが必要という前提に照らすと、毎秒200件のリクエストを190 RCUでは賄い切れません。実装するとスロットリングが発生し、ゲームのレスポンスが低下します。この選択肢を選んだ方は「項目サイズが4KB未満なら1RCU」という換算を忘れていないか確認しましょう。
- 225 RCU: 200 RCU で足りる状況に225 RCUを割り当てると、25 RCU分のコストが無駄になります。オートスケーリングでバーストを許容する設計なら余裕を持たせてもよいものの、「最小値を問う」設問条件には合致しません。リザーブドキャパシティやオンデマンドモードの切り替えと絡めて、コスト最適化の観点を復習すると理解が深まります。
- 400 RCU: 必要量の2倍を割り当てているためコストがさらに高くなります。ピークが200 RCUと分かっている場合、プロビジョンドで400 RCUを設定する合理性は薄いです。この選択肢を選んだ方は「RCUの計算方法」と「オートスケーリングやオンデマンドモードでピークを吸収する方法」の区別を再確認しましょう。

·この問題の論点
この問題は DynamoDB のプロビジョンドキャパシティ計算、特に「項目サイズ」「強い整合性読み込み」「RCU換算」の知識を問う問題です。

Q6 【DVA-31】音楽配信アプリでは再生セッションのメタ情報をAmazon DynamoDB に保存している。
ユーザーIDをパーティションキーとするグローバルセカンダリインデックス(GSI)に、過去12時間の再生イベントが書き込まれる。
モバイルアプリは同ーユーザーから1分間に数千件の再生位置更新を送信し、直近12時間以内に再生した楽曲一覧を取得する必要がある。

運用要件:
·読み取りコストを極力抑える。
·書き込み直後のデータが数秒~1分遅れて可視化されても問題ない。
·全件走査は避け、インデックスで効率的に絞り込む。
·テーブルとインデックスはプロビジョンドキャパシティで運用しており、余分な RCUは確保しない。

これらの条件を満たす最適な DynamoDB API 操作と読み取り整合性の組み合わせはどれか。

1. GSIへ Query を発行し、強い整合性読み取り(Strongly consistent read)を使用する。
2. GSIへ Scan を発行し、強い整合性読み取りを使用する。
3. GSIへ Query を発行し、結果整合性読み取り(Eventually consistent read)を使用する。
4. GSIへ Scan を発行し、結果整合性読み取りを使用する。

A6 3

該当選択肢が適切である理由

- Queryはパーティションキーを指定して必要な項目だけを取得するため、読み取りリクエストユニット(RCU)を最小化できます。
- 結果整合性読み取りは、強い整合性読み取りの1/2の RCUで済むため、さらにコストを抑えられます。
- 本シナリオでは「書き込み直後のデータが数秒~1分遅れて可視化されても問題ない」という要件があるため、結果整合性で許容範囲内です。
- したがって、Query と結果整合性読み取りの組み合わせが、運用要件に最適です。

他の選択肢が不適である理由

- GSIへQuery を発行し、強い整合性読み取り(Strongly consistent read)を使用する: Query 自体は効率的ですが、強い整合性読み取りは結果整合性読み取りの2倍のRCUを消費します。今回の要件は「読み取りコストを極力抑える」であり、「書き込み直後のデータが多少遅れても構わない」と明示されています。したがって、強い整合性を選ぶとコスト要件を満たせません。この選択をした方は「結果整合性でも秒~分単位の遅延しか発生しない」点と、「RCU単価が半減する」点を復習しましょう。
- GSIへScanを発行し、結果整合性読み取りを使用する: Scan はテーブルまたはインデックス全体を読み取るため、パーティションキーで絞り込める場合に比べてRCU消費が大きく跳ね上がります。プロビジョンドキャパシティで「余分なRCUは確保しない」という前提と矛盾します。結果整合性を採用しても、Scan で生じる大量の読み取りは抑えきれません。この選択をした方はQuery とScan の違い、特にアクセスパターンが明確なときは必ずQueryを使うというベストプラクティスを確認してください。
- GSIへScanを発行し、強い整合性読み取りを使用する: 最もコストが高い組み合わせです。Scan によりインデックス全項目を読み取るうえ、強い整合性読み取りでRCU がさらに倍増します。読み取りコスト抑制という要件と完全に逆行します。また、強い整合性は今回不要であり、この方法を選択する理由は一切ありません。この選択肢を選んだ場合は「Query とScanの使い分け」および「整合性レベルとコストの関係」を重点的に復習しましょう。

·この問題の論点
この問題は、DynamoDB の
1)Query と Scan の違いによる読み取り効率、
2)強い整合性読み取りと結果整合性読み取りのコスト·遅延トレードオフ、
3)運用コスト最適化(RCU/WCU の節約)
についての理解を問う問題です。

Q7 【DVA-32】製造ラインのセンサーログ用ストレージを設計している。開発チームは次のアクセスパターンを満たす必要がある。
·リクエスト1 sensorld とlogld を指定して単一ログをサブミリ秒で取り出す
·リクエスト2 logType を指定し当日に発生した同種のログを高頻度で一覧する

追加条件
-各レコードは sensorld(X)と logld(Y)の組み合わせで一意
-logType(Z)による検索が非常に多い
-レコードごとに可変長の追加パラメータを保持したい
-読み取りスループットは急増に合わせ自動スケーリングできること

もっとも効率的にこれらの要件を満たすデータモデルはどれか。

1. Amazon OpenSearch Service-インデックスを日付単位でロールオーバーし、sensorld と logld をフィールド化-logType集計は得意だが、単一ドキュメント取得はシャード分散によりレイテンシ増加が懸念
2. Amazon Aurora Serverless(PostgreSQL 互換)-sensorld とlogld の複合主キーを設定-可変項目は JSONB カラムに格納し、logType用インデックスを作成しないため検索は全表走査
3. Amazon DynamoDB-パーティションキー:sensorld、ソートキー:logld-logType をパーティションキーとする GSI を用意してリクエスト2を実装-スキーマレス構造のため追加パラメータをそのまま属性とし
て保存
4. Amazon DynamoDB-パーティションキー:logType、ソートキー:発生タイムスタンプ-リクエスト1は FilterExpression を伴う全件スキャンとなるが Auto Scaling で吸収する想定

## DVA-3.1-AWS にデプロイするアプリケーションアーティファクトの準備

Q1 【DVA-127】本番リージョンでは、社内経理システムの Web/AP サーバー群を、セキュリティ強化済みゴールデン AMI を指定した AWS CloudFormation スタックで運用している。耐障害性向上のため、まったく同じスタックを別リージョンに作成しようとしたところ、テンプレートに記載した Imageld が見つからずスタック作成が失敗した。
カスタム AMI の継続利用が必須であり、追加作業量と今後の運用コストを最小限に抑えたい。最適な対策はどれか。

1. 元リージョンのカスタム AMI をコピーして移行先リージョンに複製し、コピー後に発行される AMI ID をパラメータとして参照するようテンプレートを修正し、そのテンプレートでデプロイする
2. 移行先リージョンで改めて AMI をビルドし、その AMI ID をハードコードしたリージョン専用の CloudFormation テンプレートを用意する
3. CloudFormation を使わず、EC2 ウィザードやユーザーデータを用いて手作業でサーバーを立ち上げ、設定をコピーする
4. テンプレートの Imageld を公開されている Amazon Linux 2023 AMI に差し替え、カスタム AMI の利用を取りやめる
